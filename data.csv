Page Count,Url,Info
1,https://en.wikipedia.org/wiki/Web_scraping,"Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.
Scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, extraction can take place. The content of a page may be parsed, searched and reformatted, and its data copied into a spreadsheet or loaded into a database. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be finding and copying names and telephone numbers, companies and their URLs, or e-mail addresses to a list (contact scraping).
As well as contact scraping, web scraping is used as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup, and web data integration.
Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. As a result, specialized tools and software have been developed to facilitate the scraping of web pages.
Newer forms of web scraping involve monitoring data feeds from web servers.  For example, JSON is commonly used as a transport mechanism between the client and the web server.
There are methods that some websites use to prevent web scraping, such as detecting and disallowing bots from crawling (viewing) their pages. In response, there are web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing 

The history of web scraping dates back nearly to the time when the World Wide Web was born.
After the birth of the World Wide Web in 1989, the first web robot,[2] World Wide Web Wanderer, was created in June 1993, which was intended only to measure the size of the web.
In December 1993, the first crawler-based web search engine, JumpStation, was launched. As there were fewer websites available on the web, search engines at that time used to rely on human administrators to collect and format links. In comparison, JumpStation was the first WWW search engine to rely on a web robot.
In 2000, the first Web API and API crawler were created. An API (Application Programming Interface) is an interface that makes it much easier to develop a program by providing the building blocks. In 2000, Salesforce and eBay launched their own API, with which programmers could access and download some of the data available to the public. Since then, many websites offer web APIs for people to access their public database.Web scraping is the process of automatically mining data or collecting information from the World Wide Web. It is a field with active developments sharing a common goal with the semantic web vision, an ambitious initiative that still requires breakthroughs in text processing, semantic understanding, artificial intelligence and human-computer interactions.
The simplest form of web scraping is manually copying and pasting data from a web page into a text file or spreadsheet. Sometimes even the best web-scraping technology cannot replace a human's manual examination and copy-and-paste, and sometimes this may be the only workable solution when the websites for scraping explicitly set up barriers to prevent machine automation.
A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages (for instance Perl or Python).
Static and dynamic web pages can be retrieved by posting HTTP requests to the remote web server using socket programming.
Many websites have large collections of pages generated dynamically from an underlying structured source like a database. Data of the same category are typically encoded into similar pages by a common script or template. In data mining, a program that detects such templates in a particular information source, extracts its content and translates it into a relational form, is called a wrapper. Wrapper generation algorithms assume that input pages of a wrapper induction system conform to a common template and that they can be easily identified in terms of a URL common scheme.[3] Moreover, some semi-structured data query languages, such as XQuery and the HTQL, can be used to parse HTML pages and to retrieve and transform page content.
By embedding a full-fledged web browser, such as the Internet Explorer or the Mozilla browser control, programs can retrieve the dynamic content generated by client-side scripts. These browser controls also parse web pages into a DOM tree, based on which programs can retrieve parts of the pages. Languages such as Xpath can be used to parse the resulting DOM tree.
There are several companies that have developed vertical specific harvesting platforms. These platforms create and monitor a multitude of ""bots"" for specific verticals with no ""man in the loop"" (no direct human involvement), and no work related to a specific target site. The preparation involves establishing the knowledge base for the entire vertical and then the platform creates the bots automatically. The platform's robustness is measured by the quality of the information it retrieves (usually number of fields) and its scalability (how quick it can scale up to hundreds or thousands of sites). This scalability is mostly used to target the Long Tail of sites that common aggregators find complicated or too labor-intensive to harvest content from.
The pages being scraped may embrace metadata or semantic markups and annotations, which can be used to locate specific data snippets. If the annotations are embedded in the pages, as Microformat does, this technique can be viewed as a special case of DOM parsing. In another case, the annotations, organized into a semantic layer,[4] are stored and managed separately from the web pages, so the scrapers can retrieve data schema and instructions from this layer before scraping the pages.
There are efforts using machine learning and computer vision that attempt to identify and extract information from web pages by interpreting pages visually as a human being might.[5]
There are many software tools available that can be used to customize web-scraping solutions. This software may attempt to automatically recognize the data structure of a page or provide a recording interface that removes the necessity to manually write web-scraping code, or some scripting functions that can be used to extract and transform content, and database interfaces that can store the scraped data in local databases. Some web scraping software can also be used to extract data from an API directly.
The legality of web scraping varies across the world. In general, web scraping may be against the terms of service of some websites, but the enforceability of these terms is unclear.[6]
In the United States, website owners can use three major legal claims to prevent undesired web scraping: (1) copyright infringement (compilation), (2) violation of the Computer Fraud and Abuse Act (""CFAA""), and (3) trespass to chattel.[7] However, the effectiveness of these claims relies upon meeting various criteria, and the case law is still evolving. For example, with regard to copyright, while outright duplication of original expression will in many cases be illegal, in the United States the courts ruled in Feist Publications v. Rural Telephone Service that duplication of facts is allowable.
U.S. courts have acknowledged that users of ""scrapers"" or ""robots"" may be held liable for committing trespass to chattels,[8][9] which involves a computer system itself being considered personal property upon which the user of a scraper is trespassing. The best known of these cases, eBay v. Bidder's Edge, resulted in an injunction ordering Bidder's Edge to stop accessing, collecting, and indexing auctions from the eBay web site. This case involved automatic placing of bids, known as auction sniping. However, in order to succeed on a claim of trespass to chattels, the plaintiff must demonstrate that the defendant intentionally and without authorization interfered with the plaintiff's possessory interest in the computer system and that the defendant's unauthorized use caused damage to the plaintiff. Not all cases of web spidering brought before the courts have been considered trespass to chattels.[10]
One of the first major tests of screen scraping involved American Airlines (AA), and a firm called FareChase.[11] AA successfully obtained an injunction from a Texas trial court, stopping FareChase from selling software that enables users to compare online fares if the software also searches AA's website. The airline argued that FareChase's websearch software trespassed on AA's servers when it collected the publicly available data. FareChase filed an appeal in March 2003. By June, FareChase and AA agreed to settle and the appeal was dropped.[12]
Southwest Airlines has also challenged screen-scraping practices, and has involved both FareChase and another firm, Outtask, in a legal claim. Southwest Airlines charged that the screen-scraping is Illegal since it is an example of ""Computer Fraud and Abuse"" and has led to ""Damage and Loss"" and ""Unauthorized Access"" of Southwest's site. It also constitutes ""Interference with Business Relations"", ""Trespass"", and ""Harmful Access by Computer"". They also claimed that screen-scraping constitutes what is legally known as ""Misappropriation and Unjust Enrichment"", as well as being a breach of the web site's user agreement. Outtask denied all these claims, claiming that the prevailing law, in this case, should be US Copyright law and that under copyright, the pieces of information being scraped would not be subject to copyright protection. Although the cases were never resolved in the Supreme Court of the United States, FareChase was eventually shuttered by parent company Yahoo!, and Outtask was purchased by travel expense company Concur.[13]
In 2012, a startup called 3Taps scraped classified housing ads from Craigslist. Craigslist sent 3Taps a cease-and-desist letter and blocked their IP addresses and later sued, in Craigslist v. 3Taps. The court held that the cease-and-desist letter and IP blocking was sufficient for Craigslist to properly claim that 3Taps had violated the Computer Fraud and Abuse Act.
Although these are early scraping decisions, and the theories of liability are not uniform, it is difficult to ignore a pattern emerging that the courts are prepared to protect proprietary content on commercial sites from uses which are undesirable to the owners of such sites. However, the degree of protection for such content is not settled and will depend on the type of access made by the scraper, the amount of information accessed and copied, the degree to which the access adversely affects the site owner's system and the types and manner of prohibitions on such conduct.[14]
While the law in this area becomes more settled, entities contemplating using scraping programs to access a public web site should also consider whether such action is authorized by reviewing the terms of use and other terms or notices posted on or made available through the site. In a 2010 ruling in the Cvent, Inc. v. Eventbrite, Inc. In the United States district court for the eastern district of Virginia, the court ruled that the terms of use should be brought to the users' attention In order for a browse wrap contract or license to be enforced.[15] In a 2014 case, filed in the United States District Court for the Eastern District of Pennsylvania,[16] e-commerce site QVC objected to the Pinterest-like shopping aggregator Resultly's 'scraping of QVC's site for real-time pricing data. QVC alleges that Resultly ""excessively crawled"" QVC's retail site (allegedly sending 200-300 search requests to QVC's website per minute, sometimes to up to 36,000 requests per minute) which caused QVC's site to crash for two days, resulting in lost sales for QVC.[17] QVC's complaint alleges that the defendant disguised its web crawler to mask its source IP address and thus prevented QVC from quickly repairing the problem. This is a particularly interesting scraping case because QVC is seeking damages for the unavailability of their website, which QVC claims was caused by Resultly.
In the plaintiff's web site during the period of this trial, the terms of use link are displayed among all the links of the site, at the bottom of the page as most sites on the internet. This ruling contradicts the Irish ruling described below. The court also rejected the plaintiff's argument that the browse-wrap restrictions were enforceable in view of Virginia's adoption of the Uniform Computer Information Transactions Act (UCITA)—a uniform law that many believed was in favor on common browse-wrap contracting practices.[18]
In Facebook, Inc. v. Power Ventures, Inc., a district court ruled in 2012 that Power Ventures could not scrape Facebook pages on behalf of a Facebook user. The case is on appeal, and the Electronic Frontier Foundation filed a brief in 2015 asking that it be overturned.[19][20] In Associated Press v. Meltwater U.S. Holdings, Inc., a court in the US held Meltwater liable for scraping and republishing news information from the Associated Press, but a court in the United Kingdom held in favor of Meltwater.
Internet Archive collects and distributes a significant number of publicly available web pages without being considered to be in violation of copyright laws.[citation needed]
In February 2006, the Danish Maritime and Commercial Court (Copenhagen) ruled that systematic crawling, indexing, and deep linking by portal site ofir.dk of real estate site Home.dk does not conflict with Danish law or the database directive of the European Union.[21]
In a February 2010 case complicated by matters of jurisdiction, Ireland's High Court delivered a verdict that illustrates the inchoate state of developing case law. In the case of Ryanair Ltd v Billigfluege.de GmbH, Ireland's High Court ruled Ryanair's ""click-wrap"" agreement to be legally binding. In contrast to the findings of the United States District Court Eastern District of Virginia and those of the Danish Maritime and Commercial Court, Justice Michael Hanna ruled that the hyperlink to Ryanair's terms and conditions was plainly visible, and that placing the onus on the user to agree to terms and conditions in order to gain access to online services is sufficient to comprise a contractual relationship.[22] The decision is under appeal in Ireland's Supreme Court.[23]
On April 30, 2020, the French Data Protection Authority (CNIL) released new guidelines on web scraping.[24] The CNIL guidelines made it clear that publicly available data is still personal data and cannot be repurposed without the knowledge of the person to whom that data belongs.[25]
In Australia, the Spam Act 2003 outlaws some forms of web harvesting, although this only applies to email addresses.[26][27]
Leaving a few cases dealing with IPR infringement, Indian courts have not expressly ruled on the legality of web scraping. However, since all common forms of electronic contracts are enforceable in India, violating the terms of use prohibiting data scraping will be a violation of the contract law. It will also violate the Information Technology Act, 2000, which penalizes unauthorized access to a computer resource or extracting data from a computer resource.
The administrator of a website can use various measures to stop or slow a bot. Some techniques include:
Blocking an IP address either manually or based on criteria such as geolocation and DNSRBL. This will also block all browsing from that address.
Disabling any web service API that the website's system might expose.
Bots sometimes declare who they are (using user agent strings) and can be blocked on that basis using robots.txt; 'googlebot' is an example. Other bots make no distinction between themselves and a human using a browser.
Bots can be blocked by monitoring excess traffic
Bots can sometimes be blocked with tools to verify that it is a real person accessing the site, like a CAPTCHA. Bots are sometimes coded to explicitly break specific CAPTCHA patterns or may employ third-party services that utilize human labor to read and respond in real-time to CAPTCHA challenges.
Commercial anti-bot services: Companies offer anti-bot and anti-scraping services for websites. A few web application firewalls have limited bot detection capabilities as well. However, many such solutions are not very effective.[28]
Locating bots with a honeypot or other method to identify the IP addresses of automated crawlers.
Obfuscation using CSS sprites to display such data as telephone numbers or email addresses, at the cost of accessibility to screen reader users.
Because bots rely on consistency in the front-end code of a target website, adding small variations to the HTML/CSS surrounding important data and navigation elements would require more human involvement in the initial set up of a bot and if done effectively may render the target website too difficult to scrape due to the diminished ability to automate the scraping process.
Websites can declare if crawling is allowed or not in the robots.txt file and allow partial access, limit the crawl rate, specify the optimal time to crawl and more."
2,https://en.wikipedia.org/wiki/Data_scraping,"Computer security
Automotive security
Cybercrime
Cybersex trafficking
Computer fraud
Cybergeddon
Cyberterrorism
Cyberwarfare
Electronic warfare
Information warfare
Internet security
Mobile security
Network security
Copy protection
Digital rights managementCybersex trafficking
Computer fraudAdware
Advanced persistent threat
Arbitrary code execution
Backdoors
Hardware backdoors
Code injection
Crimeware
Cross-site scripting
Cryptojacking malware
Botnets
Data breach
Drive-by download
Browser helper objects
Viruses
Data scraping
Denial of service
Eavesdropping
Email fraud
Email spoofing
Exploits
Keyloggers
Logic bombs
Time bombs
Fork bombs
Zip bombs
Fraudulent dialers
Malware
Payload
Phishing
Polymorphic engine
Privilege escalation
Ransomware
Rootkits
Bootkits
Scareware
Shellcode
Spamming
Social engineering
Screen scraping
Spyware
Software bugs
Trojan horses
Hardware Trojans
Remote access trojans
Vulnerability
Web shells
Wiper
Worms
SQL injection
Rogue security software
ZombieApplication security
Secure coding
Secure by default
Secure by design
Misuse case
Computer access control
Authentication
Multi-factor authentication
Authorization
Computer security software
Antivirus software
Security-focused operating system
Data-centric security
Code obfuscation
Data masking
Encryption
Firewall
Intrusion detection system
Host-based intrusion detection system (HIDS)
Anomaly detection
Security information and event management (SIEM)
Mobile secure gateway
Runtime application self-protectionSecure coding
Secure by default
Secure by design
Misuse caseMisuse caseAuthentication
Multi-factor authentication
AuthorizationMulti-factor authenticationAntivirus software
Security-focused operating systemHost-based intrusion detection system (HIDS)
Anomaly detectionvteData scraping is a technique where a computer program extracts data from human-readable output coming from another program.
Normally, data transfer between programs is accomplished using data structures suited for automated processing by computers, not people.  Such interchange formats and protocols are typically rigidly structured, well-documented, easily parsed, and minimize ambiguity.  Very often, these transmissions are not human-readable at all.
Thus, the key element that distinguishes data scraping from regular parsing is that the output being scraped is intended for display to an end-user, rather than as an input to another program. It is therefore usually neither documented nor structured for convenient parsing.  Data scraping often involves ignoring binary data (usually images or multimedia data), display formatting, redundant labels, superfluous commentary, and other information which is either irrelevant or hinders automated processing.
Data scraping is most often done either to interface to a legacy system, which has no other mechanism which is compatible with current hardware, or to interface to a third-party system which does not provide a more convenient API.  In the second case, the operator of the third-party system will often see screen scraping as unwanted, due to reasons such as increased system load, the loss of advertisement revenue, or the loss of control of the information content.
Data scraping is generally considered an ad hoc, inelegant technique, often used only as a ""last resort"" when no other mechanism for data interchange is available.  Aside from the higher programming and processing overhead, output displays intended for human consumption often change structure frequently.  Humans can cope with this easily, but a computer program will fail. Depending on the quality and the extent of error handling logic present in the computer, this failure can result in error messages, corrupted output or even program crashes.
Although the use of physical ""dumb terminal"" IBM 3270s is slowly diminishing, as more and more mainframe applications acquire Web interfaces, some Web applications merely continue to use the technique of screen scraping to capture old screens and transfer the data to modern front-ends.[1]
Screen scraping is normally associated with the programmatic collection of visual data from a source, instead of parsing data as in web scraping. Originally, screen scraping referred to the practice of reading text data from a computer display terminal's screen. This was generally done by reading the terminal's memory through its auxiliary port, or by connecting the terminal output port of one computer system to an input port on another. The term screen scraping is also commonly used to refer to the bidirectional exchange of data. This could be the simple cases where the controlling program navigates through the user interface, or more complex scenarios where the controlling program is entering data into an interface meant to be used by a human.
As a concrete example of a classic screen scraper, consider a hypothetical legacy system dating from the 1960s—the dawn of computerized data processing. Computer to user interfaces from that era were often simply text-based dumb terminals which were not much more than virtual teleprinters (such systems are still in use today[update], for various reasons). The desire to interface such a system to more modern systems is common.  A robust solution will often require things no longer available, such as source code, system documentation, APIs, or programmers with experience in a 50-year-old computer system.  In such cases, the only feasible solution may be to write a screen scraper that ""pretends"" to be a user at a terminal.  The screen scraper might connect to the legacy system via Telnet, emulate the keystrokes needed to navigate the old user interface, process the resulting display output, extract the desired data, and pass it on to the modern system. A sophisticated and resilient implementation of this kind, built on a platform providing the governance and control required by a major enterprise—e.g. change control, security, user management, data protection, operational audit, load balancing, and queue management, etc.—could be said to be an example of robotic process automation software, called RPA or RPAAI for self-guided RPA 2.0 based on artificial intelligence.
In the 1980s, financial data providers such as Reuters, Telerate, and Quotron displayed data in 24×80 format intended for a human reader. Users of this data, particularly investment banks, wrote applications to capture and convert this character data as numeric data for inclusion into calculations for trading decisions without re-keying the data. The common term for this practice, especially in the United Kingdom, was page shredding, since the results could be imagined to have passed through a paper shredder. Internally Reuters used the term 'logicized' for this conversion process, running a sophisticated computer system on VAX/VMS called the Logicizer.[2]
More modern screen scraping techniques include capturing the bitmap data from the screen and running it through an OCR engine, or for some specialised automated testing systems, matching the screen's bitmap data against expected results.[3] This can be combined in the case of GUI applications, with querying the graphical controls by programmatically obtaining references to their underlying programming objects. A sequence of screens is automatically captured and converted into a database.
Another modern adaptation to these techniques is to use, instead of a sequence of screens as input, a set of images or PDF files, so there are some overlaps with generic ""document scraping"" and report mining techniques.
There are many tools that can be used for screen scraping.[4]
Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form.  However, most web pages are designed for human end-users and not for ease of automated use. Because of this, tool kits that scrape web content were created. A web scraper is an API or tool to extract data from a website.[5] Companies like Amazon AWS and Google provide web scraping tools, services, and public data available free of cost to end-users. Newer forms of web scraping involve listening to data feeds from web servers.  For example, JSON is commonly used as a transport storage mechanism between the client and the webserver.
Recently, companies have developed web scraping systems that rely on using techniques in DOM parsing, computer vision and natural language processing to simulate the human processing that occurs when viewing a webpage to automatically extract useful information.[6][7]
Large websites usually use defensive algorithms to protect their data from web scrapers and to limit the number of requests an IP or IP network may send. This has caused an ongoing battle between website developers and scraping developers.[8]
Report mining is the extraction of data from human-readable computer reports. Conventional data extraction requires a connection to a working source system, suitable connectivity standards or an API, and usually complex querying. By using the source system's standard reporting options, and directing the output to a spool file instead of to a printer, static reports can be generated suitable for offline analysis via report mining.[9] This approach can avoid intensive CPU usage during business hours, can minimise end-user licence costs for ERP customers, and can offer very rapid prototyping and development of custom reports. Whereas data scraping and web scraping involve interacting with dynamic output, report mining involves extracting data from files in a human-readable format, such as HTML, PDF, or text. These can be easily generated from almost any system by intercepting the data feed to a printer. This approach can provide a quick and simple route to obtaining data without the need to program an API to the source system.
"
3,https://en.wikipedia.org/wiki/Computer_program,"A computer program is a sequence or set of instructions in a programming language for a computer to execute. Computer programs are one component of software, which also includes documentation and other intangible components.[1]
A computer program in its human-readable form is called source code. Source code needs another computer program to execute because computers can only execute their native machine instructions. Therefore, source code may be translated to machine instructions using the language's compiler. (Assembly language programs are translated using an assembler.) The resulting file is called an executable. Alternatively, source code may execute within the language's interpreter.[2]
If the executable is requested for execution, then the operating system loads it into memory and starts a process.[3] The central processing unit will soon switch to this process so it can fetch, decode, and then execute each machine instruction.[4]
If the source code is requested for execution, then the operating system loads the corresponding interpreter into memory and starts a process. The interpreter then loads the source code into memory to translate and execute each statement.[2] Running the source code is slower than running an executable.[a] Moreover, the interpreter must be installed on the computer.
The ""Hello, World!"" program is used to illustrate a language's basic syntax. The syntax of the language BASIC (1964) was intentionally limited to make the language easy to learn.[5] For example, variables are not declared before being used.[6] Also, variables are automatically initialized to zero.[6] Here is an example computer program, in Basic, to average a list of numbers:[7]
Once the mechanics of basic computer programming are learned, more sophisticated and powerful languages are available to build large computer systems.[8]
Improvements in software development are the result of improvements in computer hardware. At each stage in hardware's history, the task of computer programming changed dramatically.
In 1837, Charles Babbage was inspired by Jacquard's loom to attempt to build the Analytical Engine.[9]
The names of the components of the calculating device were borrowed from the textile industry. In the textile industry, yarn was brought from the store to be milled. The device had a ""store"" which consisted of memory to hold 1,000 numbers of 50 decimal digits each.[10] Numbers from the ""store"" were transferred to the ""mill"" for processing. It was programmed using two sets of perforated cards. One set directed the operation and the other set inputted the variables.[9][11] However, after more than 17,000 pounds of the British government's money, the thousands of cogged wheels and gears never fully worked together.[12]
Ada Lovelace worked for Charles Babbage to create a description of the Analytical Engine (1843).[13] The description contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first computer program.[12]
In 1936, Alan Turing introduced the Universal Turing machine, a theoretical device that can model every computation.[14]
It is a finite-state machine that has an infinitely long read/write tape. The machine can move the tape back and forth, changing its contents as it performs an algorithm. The machine starts in the initial state, goes through a sequence of steps, and halts when it encounters the halt state.[15] All present-day computers are Turing complete.[16]
The Electronic Numerical Integrator And Computer (ENIAC) was built between July 1943 and Fall 1945. It was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together.[17] Its 40 units weighed 30 tons, occupied 1,800 square feet (167 m2), and consumed $650 per hour (in 1940s currency) in electricity when idle.[17] It had 20 base-10 accumulators. Programming the ENIAC took up to two months.[17] Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels by plugging heavy black cables into plugboards. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week.[18] It ran from 1947 until 1955 at Aberdeen Proving Ground, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns.[19]
Instead of plugging in cords and turning switches, a stored-program computer loads its instructions into memory just like it loads its data into memory.[20] As a result, the computer could be programmed quickly and perform calculations at very fast speeds.[21] Presper Eckert and John Mauchly built the ENIAC. The two engineers introduced the stored-program concept in a three-page memo dated February 1944.[22] Later, in September 1944, Dr. John von Neumann began working on the ENIAC project. On June 30, 1945, von Neumann published the First Draft of a Report on the EDVAC which equated the structures of the computer with the structures of the human brain.[21] The design became known as the von Neumann architecture. The architecture was simultaneously deployed in the constructions of the EDVAC and EDSAC computers in 1949.[23]
The IBM System/360 (1964) was a family of computers, each having the same instruction set architecture. The Model 20 was the smallest and least expensive. Customers could upgrade and retain the same application software.[24] The Model 195 was the most premium.  Each System/360 model featured multiprogramming[24]—having multiple processes in memory at once. When one process was waiting for input/output, another could compute.
IBM planned for each model to be programmed using PL/1.[25] A committee was formed that included COBOL, Fortran and ALGOL programmers. The purpose was to develop a language that was comprehensive, easy to use, extendible, and would replace Cobol and Fortran.[25] The result was a large and complex language that took a long time to compile.[26]
Computers manufactured until the 1970s had front-panel switches for manual programming.[27] The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were automatically inputted via paper tape, punched cards or magnetic-tape. After the medium was loaded, the starting address was set via switches, and the execute button was pressed.[27]
A major milestone in software development was the invention of the Very Large Scale Integration (VLSI) circuit (1964).[28] Following World War II, tube-based technology was replaced with point-contact transistors (1947) and bipolar junction transistors (late 1950s) mounted on a circuit board.[28] During the 1960s, the aerospace industry replaced the circuit board with an integrated circuit chip.[28]
Robert Noyce, co-founder of Fairchild Semiconductor (1957) and Intel (1968), achieved a technological improvement to refine the production of field-effect transistors (1963).[29] The goal is to alter the electrical resistivity and conductivity of a semiconductor junction. First, naturally occurring silicate minerals are converted into polysilicon rods using the Siemens process.[30] The Czochralski process then converts the rods into a monocrystalline silicon, boule crystal.[31] The crystal is then thinly sliced to form a wafer substrate. The planar process of photolithography then integrates unipolar transistors, capacitors, diodes, and resistors onto the wafer to build a matrix of metal–oxide–semiconductor (MOS) transistors.[32][33] The MOS transistor is the primary component in integrated circuit chips.[29]
Originally, integrated circuit chips had their function set during manufacturing. During the 1960s, controlling the electrical flow migrated to programming a matrix of read-only memory (ROM). The matrix resembled a two-dimensional array of fuses.[28] The process to embed instructions onto the matrix was to burn out the unneeded connections.[28] There were so many connections, firmware programmers wrote a computer program on another chip to oversee the burning.[28] The technology became known as Programmable ROM. In 1971, Intel installed the computer program onto the chip and named it the Intel 4004 microprocessor.[34]
The terms microprocessor and central processing unit (CPU) are now used interchangeably. However, CPUs predate microprocessors. For example, the IBM System/360 (1964) had a CPU made from circuit boards containing discrete components on ceramic substrates.[35]
The Intel 4004 (1971) was a 4-bit microprocessor designed to run the Busicom calculator. Five months after its release, Intel released the Intel 8008, an 8-bit microprocessor. Bill Pentz led a team at Sacramento State to build the first microcomputer using the Intel 8008: the Sac State 8008 (1972).[36] Its purpose was to store patient medical records. The computer supported a disk operating system to run a Memorex, 3-megabyte, hard disk drive.[28] It had a color display and keyboard that was packaged in a single console. The disk operating system was programmed using IBM's Basic Assembly Language (BAL). The medical records application was programmed using a BASIC interpreter.[28] However, the computer was an evolutionary dead-end because it was extremely expensive. Also, it was built at a public university lab for a specific purpose.[36] Nonetheless, the project contributed to the development of the Intel 8080 (1974) instruction set.[28]
In 1978, the modern software development environment began when Intel upgraded the Intel 8080 to the Intel 8086. Intel simplified the Intel 8086 to manufacture the cheaper Intel 8088.[37] IBM embraced the Intel 8088 when they entered the personal computer market (1981). As consumer demand for personal computers increased, so did Intel's microprocessor development. The succession of development is known as the x86 series. The x86 assembly language is a family of backward-compatible machine instructions. Machine instructions created in earlier microprocessors were retained throughout microprocessor upgrades. This enabled consumers to purchase new computers without having to purchase new application software.  The major categories of instructions are:[b]
Memory instructions to set and access numbers and strings in random-access memory.
Integer arithmetic logic unit (ALU) instructions to perform the primary arithmetic operations on integers.
Floating point ALU instructions to perform the primary arithmetic operations on real numbers.
Call stack instructions to push and pop words needed to allocate memory and interface with functions.
Single instruction, multiple data (SIMD) instructions[c] to increase speed when multiple processors are available to perform the same algorithm on an array of data.VLSI circuits enabled the programming environment to advance from a computer terminal (until the 1990s) to a graphical user interface (GUI) computer. Computer terminals limited programmers to a single shell running in a command-line environment. During the 1970s, full-screen source code editing became possible through a text-based user interface. Regardless of the technology available, the goal is to program in a programming language.
Programming language features exist to provide building blocks to be combined to express programming ideals.[38] Ideally, a programming language should:[38]
express ideas directly in the code.
express independent ideas independently.
express relationships among ideas directly in the code.
combine ideas freely.
combine ideas only where combinations make sense.
express simple ideas simply.The programming style of a programming language to provide these building blocks may be categorized into programming paradigms.[39] For example, different paradigms may differentiate:[39]
procedural languages, functional languages, and logical languages.
different levels of data abstraction.
different levels of class hierarchy.
different levels of input datatypes, as in container types and generic programming.Each of these programming styles has contributed to the synthesis of different programming languages.[39]
A programming language is a set of keywords, symbols, identifiers, and rules by which programmers can communicate instructions to the computer.[40] They follow a set of rules called a syntax.[40]
Keywords are reserved words to form declarations and statements.
Symbols are characters to form operations, assignments, control flow, and delimiters.
Identifiers are words created by programmers to form constants, variable names, structure names, and function names.
Syntax Rules are defined in the Backus–Naur form.Programming languages get their basis from formal languages.[41] The purpose of defining a solution in terms of its formal language is to generate an algorithm to solve the underlining problem.[41] An algorithm is a sequence of simple instructions that solve a problem.[42]
The evolution of programming language began when the EDSAC (1949) used the first stored computer program in its von Neumann architecture.[43] Programming the EDSAC was in the first generation of programming language.
The first generation of programming language is machine language.[44] Machine language requires the programmer to enter instructions using instruction numbers called machine code. For example, the ADD operation on the PDP-11 has instruction number 24576.[45]The second generation of programming language is assembly language.[44] Assembly language allows the programmer to use mnemonic instructions instead of remembering instruction numbers. An assembler translates each assembly language mnemonic into its machine language number. For example, on the PDP-11, the operation 24576 can be referenced as ADD in the source code.[45] The four basic arithmetic operations have assembly instructions like ADD, SUB, MUL, and DIV.[45] Computers also have instructions like DW (Define Word) to reserve memory cells. Then the MOV instruction can copy integers between registers and memory.The basic structure of an assembly language statement is a label, operation, operand, and comment.[46]Labels allow the programmer to work with variable names. The assembler will later translate labels into physical memory addresses.
Operations allow the programmer to work with mnemonics. The assembler will later translate mnemonics into instruction numbers.
Operands tell the assembler which data the operation will process.
Comments allow the programmer to articulate a narrative because the instructions alone are vague.The third generation of programming language uses compilers and interpreters to execute computer programs. The distinguishing feature of a third generation language is its independence from particular hardware.[48] Early languages include Fortran (1958), COBOL (1959), ALGOL (1960), and BASIC (1964).[44] In 1973, the C programming language emerged as a high-level language that produced efficient machine language instructions.[49] Whereas third-generation languages historically generated many machine instructions for each statement,[50] C has statements that may generate a single machine instruction.[d] Moreover, an optimizing compiler might overrule the programmer and produce fewer machine instructions than statements. Today, an entire paradigm of languages fill the imperative, third generation spectrum.The fourth generation of programming language emphasizes what output results are desired, rather than how programming statements should be constructed.[44] Declarative languages attempt to limit side effects and allow programmers to write code with relatively few errors.[44] One popular fourth generation language is called Structured Query Language (SQL).[44] Database developers no longer need to process each database record one at a time. Also, a simple instruction can generate output records without having to understand how it's retrieved.Imperative languages specify a sequential algorithm using declarations, expressions, and statements:[51]
A declaration introduces a variable name to the computer program and assigns it to a datatype[52] – for example: var x: integer;
An expression yields a value – for example: 2 + 2 yields 4
A statement might assign an expression to a variable or use the value of a variable to alter the program's control flow – for example: x := 2 + 2; if x = 4 then do_something();FORTRAN (1958) was unveiled as ""The IBM Mathematical FORmula TRANslating system."" It was designed for scientific calculations, without string handling facilities. Along with declarations, expressions, and statements, it supported:
arrays.
subroutines.
""do"" loops.It succeeded because:
programming and debugging costs were below computer running costs.
it was supported by IBM.
applications at the time were scientific.[53]However, non-IBM vendors also wrote Fortran compilers, but with a syntax that would likely fail IBM's compiler.[53] The American National Standards Institute (ANSI) developed the first Fortran standard in 1966. In 1978, Fortran 77 became the standard until 1991. Fortran 90 supports:
records.
pointers to arrays.COBOL (1959) stands for ""COmmon Business Oriented Language."" Fortran manipulated symbols. It was soon realized that symbols didn't need to be numbers, so strings were introduced.[54] The US Department of Defense influenced COBOL's development, with Grace Hopper being a major contributor. The statements were English-like and verbose. The goal was to design a language so managers could read the programs. However, the lack of structured statements hindered this goal.[55]
COBOL's development was tightly controlled, so dialects didn't emerge to require ANSI standards. As a consequence, it wasn't changed for 15 years until 1974. The 1990s version did make consequential changes, like object-oriented programming.[55]
ALGOL (1960) stands for ""ALGOrithmic Language."" It had a profound influence on programming language design.[56] Emerging from a committee of European and American programming language experts, it used standard mathematical notation and had a readable structured design. Algol was first to define its syntax using the Backus–Naur form.[56] This led to syntax-directed compilers. It added features like:
block structure, where variables were local to their block.
arrays with variable bounds.
""for"" loops.
functions.
recursion.[56]Algol's direct descendants include Pascal, Modula-2, Ada, Delphi and Oberon on one branch. On another branch the descendants include C, C++ and Java.[56]
BASIC (1964) stands for ""Beginner's All-Purpose Symbolic Instruction Code."" It was developed at Dartmouth College for all of their students to learn.[7] If a student didn't go on to a more powerful language, the student would still remember Basic.[7] A Basic interpreter was installed in the microcomputers manufactured in the late 1970s. As the microcomputer industry grew, so did the language.[7]
Basic pioneered the interactive session.[7] It offered operating system commands within its environment:
The 'new' command created an empty slate.
Statements evaluated immediately.
Statements could be programmed by preceding them with a line number.
The 'list' command displayed the program.
The 'run' command executed the program.However, the Basic syntax was too simple for large programs.[7] Recent dialects added structure and object-oriented extensions. Microsoft's Visual Basic is still widely used and produces a graphical user interface.[6]
C programming language (1973) got its name because the language BCPL was replaced with B, and AT&T Bell Labs called the next version ""C."" Its purpose was to write the UNIX operating system.[49] C is a relatively small language, making it easy to write compilers. Its growth mirrored the hardware growth in the 1980s.[49] Its growth also was because it has the facilities of assembly language, but uses a high-level syntax. It added advanced features like:
inline assembler.
arithmetic on pointers.
pointers to functions.
bit operations.
freely combining complex operators.[49]C allows the programmer to control which region of memory data is to be stored. Global variables and static variables require the fewest clock cycles to store. The stack is automatically used for the standard variable declarations. Heap memory is returned to a pointer variable from the malloc() function.
The global and static data region is located just above the program region. (The program region is technically called the text region. It's where machine instructions are stored.)The global and static data region is technically two regions.[57] One region is called the initialized data segment, where variables declared with default values are stored. The other region is called the block started by segment, where variables declared without default values are stored.
Variables stored in the global and static data region have their addresses set at compile-time. They retain their values throughout the life of the process.The global and static region stores the global variables that are declared on top of (outside) the main() function.[58] Global variables are visible to main() and every other function in the source code.Local variables declared using the static prefix are also stored in the global and static data region.[57] Unlike global variables, static variables are only visible within the function or block. Static variables always retain their value. An example usage would be the function int increment_counter(){ static int counter = 0; counter++; return counter;}The stack region is a contiguous block of memory located near the top memory address.[60] Variables placed in the stack are populated from top to bottom.[e][60] A stack pointer is a special-purpose register that keeps track of the last memory address populated.[60] Variables are placed into the stack via the assembly language PUSH instruction. Therefore, the addresses of these variables are set during runtime. The method for stack variables to lose their scope is via the POP instruction.Local variables declared without the static prefix, including formal parameter variables,[61] are called automatic variables[58] and are stored in the stack.[57] They are visible inside the function or block and lose their scope upon exiting the function or block.The heap region is located below the stack.[57] It is populated from the bottom to the top. The operating system manages the heap using a heap pointer and a list of allocated memory blocks.[62] Like the stack, the addresses of heap variables are set during runtime. An out of memory error occurs when the heap pointer and the stack pointer meet.C provides the malloc() library function to allocate heap memory.[63] Populating the heap with data is an additional copy function. Variables stored in the heap are economically passed to functions using pointers. Without pointers, the entire block of data would have to be passed to the function via the stack.In the 1970s, software engineers needed language support to break large projects down into modules.[64] One obvious feature was to decompose large projects physically into separate files. A less obvious feature was to decompose large projects logically into abstract datatypes.[64] At the time, languages supported concrete (scalar) datatypes like integer numbers, floating-point numbers, and strings of characters. Concrete datatypes have their representation as part of their name.[65] Abstract datatypes are structures of concrete datatypes, with a new name assigned. For example, a list of integers could be called integer_list.
In object-oriented jargon, abstract datatypes are called classes. However, a class is only a definition; no memory is allocated. When memory is allocated to a class and bound to an identifier, it's called an object.[66]
Object-oriented imperative languages developed by combining the need for classes and the need for safe functional programming.[67] A function, in an object-oriented language, is assigned to a class. An assigned function is then referred to as a method, member function, or operation. Object-oriented programming is executing operations on objects.[68]
Object-oriented languages support a syntax to model subset/superset relationships. In set theory, an element of a subset inherits all the attributes contained in the superset. For example, a student is a person. Therefore, the set of students is a subset of the set of persons. As a result, students inherit all the attributes common to all persons. Additionally, students have unique attributes that other people don't have. Object-oriented languages model subset/superset relationships using inheritance.[69] Object-oriented programming became the dominant language paradigm by the late 1990s.[64]
C++ (1985) was originally called ""C with Classes.""[70] It was designed to expand C's capabilities by adding the object-oriented facilities of the language Simula.[71]
An object-oriented module is composed of two files. The definitions file is called the header file. Here is a C++ header file for the GRADE class in a simple school application:
A constructor operation is a function with the same name as the class name.[72] It is executed when the calling operation executes the new statement.
A module's other file is the source file. Here is a C++ source file for the GRADE class in a simple school application:
Here is a C++ header file for the PERSON class in a simple school application:
Here is a C++ source file for the PERSON class in a simple school application:
Here is a C++ header file for the STUDENT class in a simple school application:
Here is a C++ source file for the STUDENT class in a simple school application:
Here is a driver program for demonstration:
Here is a makefile to compile everything:
Imperative languages have one major criticism: assigning an expression to a non-local variable may produce an unintended side effect.[73] Declarative languages generally omit the assignment statement and the control flow. They describe what computation should be performed and not how to compute it. Two broad categories of declarative languages are functional languages and logical languages.
The principle behind a functional language is to use lambda calculus as a guide for a well defined semantic.[74] In mathematics, a function is a rule that maps elements from an expression to a range of values. Consider the function:
times_10(x) = 10 * x
The expression 10 * x is mapped by the function times_10() to a range of values. One value happens to be 20. This occurs when x is 2. So, the application of the function is mathematically written as:
times_10(2) = 20
A functional language compiler will not store this value in a variable. Instead, it will push the value onto the computer's stack before setting the program counter back to the calling function. The calling function will then pop the value from the stack.[75]
Imperative languages do support functions. Therefore, functional programming can be achieved in an imperative language, if the programmer uses discipline. However, a functional language will force this discipline onto the programmer through its syntax. Functional languages have a syntax tailored to emphasize the what.[76]
A functional program is developed with a set of primitive functions followed by a single driver function.[73] Consider the snippet:
function max(a,b){/* code omitted */}
function min(a,b){/* code omitted */}
function difference_between_largest_and_smallest(a,b,c) {
}
The primitives are max() and min(). The driver function is difference_between_largest_and_smallest(). Executing:
put(difference_between_largest_and_smallest(10,4,7)); will output 6.
Functional languages are used in computer science research to explore new language features.[77] Moreover, their lack of side-effects have made them popular in parallel programming and concurrent programming.[78] However, application developers prefer the object-oriented features of imperative languages.[78]
Lisp (1958) stands for ""LISt Processor.""[79] It is tailored to process lists. A full structure of the data is formed by building lists of lists. In memory, a tree data structure is built. Internally, the tree structure lends nicely for recursive functions.[80] The syntax to build a tree is to enclose the space-separated elements within parenthesis. The following is a list of three elements. The first two elements are themselves lists of two elements:
((A B) (HELLO WORLD) 94)
Lisp has functions to extract and reconstruct elements.[81] The function head() returns a list containing the first element in the list. The function tail() returns a list containing everything but the first element. The function cons() returns a list that is the concatenation of other lists. Therefore, the following expression will return the list x:
cons(head(x), tail(x))
One drawback of Lisp is when many functions are nested, the parentheses may look confusing.[76] Modern Lisp environments help ensure parenthesis match. As an aside, Lisp does support the imperative language operations of the assignment statement and goto loops.[82] Also, Lisp is not concerned with the datatype of the elements at compile time.[83] Instead, it assigns (and may reassign) the datatypes at runtime. Assigning the datatype at runtime is called dynamic binding.[84] Whereas dynamic binding increases the language's flexibility, programming errors may linger until late in the software development process.[84]
Writing large, reliable, and readable Lisp programs requires forethought. If properly planned, the program may be much shorter than an equivalent imperative language program.[76] Lisp is widely used in artificial intelligence. However, its usage has been accepted only because it has imperative language operations, making unintended side-effects possible.[78]
ML (1973)[85] stands for ""Meta Language."" ML checks to make sure only data of the same type are compared with one another.[86] For example, this function has one input parameter (an integer) and returns an integer:
ML is not parenthesis-eccentric like Lisp. The following is an application of times_10():
It returns ""20 : int"". (Both the results and the datatype are returned.)
Like Lisp, ML is tailored to process lists. Unlike Lisp, each element is the same datatype.[87] Moreover, ML assigns the datatype of an element at compile-time. Assigning the datatype at compile-time is called static binding. Static binding increases reliability because the compiler checks the context of variables before they are used.[88]
Prolog (1972) stands for ""PROgramming in LOgic."" It was designed to process natural languages.[89] The building blocks of a Prolog program are objects and their relationships to other objects. Objects are built by stating true facts about them.[90]
Set theory facts are formed by assigning objects to sets. The syntax is setName(object).
Cat is an animal.Mouse is an animal.Tom is a cat.Jerry is a mouse.Adjective facts are formed using adjective(object).
Cat is big.Mouse is small.Relationships are formed using multiple items inside the parentheses. In our example we have verb(object,object) and verb(adjective,adjective).
Mouse eats cheese.Big animals eat small animals.After all the facts and relationships are entered, then a question can be asked:
Prolog's usage has expanded to become a goal-oriented language.[91] In a goal-oriented application, the goal is defined by providing a list of subgoals. Then each subgoal is defined by further providing a list of its subgoals, etc. If a path of subgoals fails to find a solution, then that subgoal is backtracked and another path is systematically attempted.[90] Practical applications include solving the shortest path problem[89] and producing family trees.[92]
Object-oriented programming is a programming method to execute operations (functions) on objects.[93] The basic idea is to group the characteristics of a phenomenon into an object container and give the container a name. The operations on the phenomenon are also grouped into the container.[93] Object-oriented programming developed by combining the need for containers and the need for safe functional programming.[94] This programming method need not be confined to an object-oriented language.[95] In an object-oriented language, an object container is called a class. In a non-object-oriented language, a data structure (which is also known as a record) may become an object container. To turn a data structure into an object container, operations need to be written specifically for the structure. The resulting structure is called an abstract datatype.[96] However, inheritance will be missing. Nonetheless, this shortcoming can be overcome.
Here is a C programming language header file for the GRADE abstract datatype in a simple school application:
The grade_new() function performs the same algorithm as the C++ constructor operation.
Here is a C programming language source file for the GRADE abstract datatype in a simple school application:
In the constructor, the function calloc() is used instead of malloc() because each memory cell will be set to zero.
Here is a C programming language header file for the PERSON abstract datatype in a simple school application:
Here is a C programming language source file for the PERSON abstract datatype in a simple school application:
Here is a C programming language header file for the STUDENT abstract datatype in a simple school application:
Here is a C programming language source file for the STUDENT abstract datatype in a simple school application:
Here is a driver program for demonstration:
Here is a makefile to compile everything:
The formal strategy to build object-oriented objects is to:[97]
Identify the objects. Most likely these will be nouns.
Identify each object's attributes. What helps to describe the object?
Identify each object's actions. Most likely these will be verbs.
Identify the relationships from object to object. Most likely these will be verbs.For example:
A person is a human identified by a name.
A grade is an achievement identified by a letter.
A student is a person who earns a grade.The syntax of a programming language is a list of production rules which govern its form.[98] A programming language's form is the correct placement of its declarations, expressions, and statements.[99] Complementing the syntax of a language are its semantics. The semantics describe the meanings attached to various syntactic constructs.[98] A syntactic construct may need a semantic description because a form may have an invalid interpretation.[100] Also, different languages might have the same syntax; however, their behaviors may be different.
The syntax of a language is formally described by listing the production rules. Whereas the syntax of a natural language is extremely complicated, a subset of the English language can have this production rule listing:[101]
The words in bold-face are known as ""non-terminals"". The words in 'single quotes' are known as ""terminals"".[102]
From this production rule listing, complete sentences may be formed using a series of replacements.[103] The process is to replace non-terminals with either a valid non-terminal or a valid terminal. The replacement process repeats until only terminals remain. One valid sentence is:
sentence
noun-phrase verb-phrase
article adjective noun verb-phrase
the adjective noun verb-phrase
the big noun verb-phrase
the big cat verb-phrase
the big cat verb noun-phrase
the big cat eats noun-phrase
the big cat eats article adjective noun
the big cat eats the adjective noun
the big cat eats the small noun
the big cat eats the small mouseHowever, another combination results in an invalid sentence:
the small mouse eats the big catTherefore, a semantic is necessary to correctly describe the meaning of an eat activity.
One production rule listing method is called the Backus–Naur form (BNF).[104] BNF describes the syntax of a language and itself has a syntax. This recursive definition is an example of a meta-language.[98] The syntax of BNF includes:
::= which translates to is made up of a[n] when a non-terminal is to its right. It translates to is when a terminal is to its right.
| which translates to or.
< and > which surround non-terminals.Using BNF, a subset of the English language can have this production rule listing:
Using BNF, a signed-integer has the production rule listing:[105]
Notice the recursive production rule:
This allows for an infinite number of possibilities. Therefore, a semantic is necessary to describe a limitation of the number of digits.
Notice the leading zero possibility in the production rules:
Therefore, a semantic is necessary to describe that leading zeros need to be ignored.
Two formal methods are available to describe semantics. They are denotational semantics and axiomatic semantics.[106]
Software engineering is a variety of techniques to produce quality software.[107] Computer programming is the process of writing or editing source code. In a formal environment, a systems analyst will gather information from managers about all the organization's processes to automate. This professional then prepares a detailed plan for the new or modified system.[108] The plan is analogous to an architect's blueprint.[108]
The systems analyst has the objective to deliver the right information to the right person at the right time.[109] The critical factors to achieve this objective are:[109]
Achieving performance objectives should be balanced with all of the costs, including:[110]
Applying a systems development process will mitigate the axiom: the later in the process an error is detected, the more expensive it is to correct.[111]
The waterfall model is an implementation of a systems development process.[112] As the waterfall label implies, the basic phases overlap each other:[113]
A computer programmer is a specialist responsible for writing or modifying the source code to implement the detailed plan.[108] A programming team is likely to be needed because most systems are too large to be completed by a single programmer.[115] However, adding programmers to a project may not shorten the completion time. Instead, it may lower the quality of the system.[115] To be effective, program modules need to be defined and distributed to team members.[115] Also, team members must interact with one another in a meaningful and effective way.[115]
Computer programmers may be programming-in-the-small: programming within a single module.[116] Chances are a module will execute modules located in other source code files. Therefore, computer programmers may be programming-in-the-large: programming modules so they will effectively couple with each other.[116] Programming-in-the-large includes contributing to the application programming interface (API).
Modular programming is a technique to refine imperative language programs. Refined programs may reduce the software size, separate responsibilities, and thereby mitigate software aging. A program module is a sequence of statements that are bounded within a block and together identified by a name.[117] Modules have a function, context, and logic:[118]
The function of a module is what it does.
The context of a module are the elements being performed upon.
The logic of a module is how it performs the function.The module's name should be derived first by its function, then by its context. Its logic should not be part of the name.[118] For example, function compute_square_root( x ) or function compute_square_root_integer( i : integer ) are appropriate module names. However, function compute_square_root_by_division( x ) is not.
The degree of interaction within a module is its level of cohesion.[118] Cohesion is a judgment of the relationship between a module's name and its function. The degree of interaction between modules is the level of coupling.[119] Coupling is a judgement of the relationship between a module's context and the elements being performed upon.
The levels of cohesion from worst to best are:[120]
Coincidental Cohesion: A module has coincidental cohesion if it performs multiple functions, and the functions are completely unrelated. For example, function read_sales_record_print_next_line_convert_to_float(). Coincidental cohesion occurs in practice if management enforces silly rules. For example, ""Every module will have between 35 and 50 executable statements.""[120]
Logical Cohesion: A module has logical cohesion if it has available a series of functions, but only one of them is executed. For example, function perform_arithmetic( perform_addition, a, b ).
Temporal Cohesion: A module has temporal cohesion if it performs functions related to time. One example, function initialize_variables_and_open_files(). Another example, stage_one(), stage_two(), ...
Procedural Cohesion: A module has procedural cohesion if it performs multiple loosely related functions. For example, function read_part_number_update_employee_record().
Communicational Cohesion: A module has communicational cohesion if it performs multiple closely related functions. For example, function read_part_number_update_sales_record().
Informational Cohesion: A module has informational cohesion if it performs multiple functions, but each function has its own entry and exit points. Moreover, the functions share the same data structure. Object-oriented classes work at this level.
Functional Cohesion: a module has functional cohesion if it achieves a single goal working only on local variables. Moreover, it may be reusable in other contexts.The levels of coupling from worst to best are:[119]
Content Coupling: A module has content coupling if it modifies a local variable of another function. COBOL used to do this with the alter verb.
Common Coupling: A module has common coupling if it modifies a global variable.
Control Coupling: A module has control coupling if another module can modify its control flow. For example, perform_arithmetic( perform_addition, a, b ). Instead, control should be on the makeup of the returned object.
Stamp Coupling: A module has stamp coupling if an element of a data structure passed as a parameter is modified. Object-oriented classes work at this level.
 Data Coupling: A module has data coupling if all of its input parameters are needed and none of them are modified. Moreover, the result of the function is returned as a single object.Data flow analysis is a design method used to achieve modules of functional cohesion and data coupling.[121] The input to the method is a data-flow diagram. A data-flow diagram is a set of ovals representing modules. Each module's name is displayed inside its oval. Modules may be at the executable level or the function level.
The diagram also has arrows connecting modules to each other. Arrows pointing into modules represent a set of inputs. Each module should have only one arrow pointing out from it to represent its single output object. (Optionally, an additional exception arrow points out.) A daisy chain of ovals will convey an entire algorithm. The input modules should start the diagram. The input modules should connect to the transform modules. The transform modules should connect to the output modules.[122]
Computer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system, which couples computer hardware with application software.[123] The purpose of the operating system is to provide an environment where application software executes in a convenient and efficient manner.[123] Both application software and system software execute utility programs. At the hardware level, a microcode program controls the circuits throughout the central processing unit.
Application software is the key to unlocking the potential of the computer system.[124] Enterprise application software bundles accounting, personnel, customer, and vendor applications. Examples include enterprise resource planning, customer relationship management, and supply chain management software.
Enterprise applications may be developed in-house as a one-of-a-kind proprietary software.[125] Alternatively, they may be purchased as off-the-shelf software. Purchased software may be modified to provide custom software. If the application is customized, then either the company's resources are used or the resources are outsourced. Outsourced software development may be from the original software vendor or a third-party developer.[126]
The potential advantages of in-house software are features and reports may be developed exactly to specification.[127] Management may also be involved in the development process and offer a level of control.[128] Management may decide to counteract a competitor's new initiative or implement a customer or vendor requirement.[129]  A merger or acquisition may necessitate enterprise software changes. The potential disadvantages of in-house software are time and resource costs may be extensive.[125] Furthermore, risks concerning features and performance may be looming.
The potential advantages of off-the-shelf software are upfront costs are identifiable, the basic needs should be fulfilled, and its performance and reliability have a track record.[125] The potential disadvantages of off-the-shelf software are it may have unnecessary features that confuse end users, it may lack features the enterprise needs, and the data flow may not match the enterprise's work processes.[125]
One approach to economically obtaining a customized enterprise application is through an application service provider.[130] Specialty companies provide hardware, custom software, and end-user support. They may speed the development of new applications because they possess skilled information system staff. The biggest advantage is it frees in-house resources from staffing and managing complex computer projects.[130] Many application service providers target small, fast-growing companies with limited information system resources.[130] On the other hand, larger companies with major systems will likely have their technical infrastructure in place. One risk is having to trust an external organization with sensitive information. Another risk is having to trust the provider's infrastructure reliability.[130]
An operating system is the low-level software that supports a computer's basic functions, such as scheduling processes and controlling peripherals.[123]
In the 1950s, the programmer, who was also the operator, would write a program and run it. After the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing.[27] More often than not the program did not work. The programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study. In the 1960s, programmers reduced the amount of wasted time by automating the operator's job. A program called an operating system was kept in the computer at all times.[131]
The term operating system may refer to two levels of software.[132] The operating system may refer to the kernel program that manages the processes, memory, and devices. More broadly, the operating system may refer to the entire package of the central software. The package includes a kernel program, command-line interpreter, graphical user interface, utility programs, and editor.[132]
The kernel's main purpose is to manage the limited resources of a computer:
The kernel program should perform process scheduling.[133] The kernel creates a process control block when a program is selected for execution. However, an executing program gets exclusive access to the central processing unit only for a time slice. To provide each user with the appearance of continuous access, the kernel quickly preempts each process control block to execute another one. The goal for system developers is to minimize dispatch latency.The kernel program should perform memory management.When the kernel initially loads an executable into memory, it divides the address space logically into regions.[134] The kernel maintains a master-region table and many per-process-region (pregion) tables—one for each running process.[134] These tables constitute the virtual address space. The master-pregion table is used to determine where its contents are located in physical memory. The pregion tables allow each process to have its own program (text) pregion, data pregion, and stack pregion.
The program pregion stores machine instructions. Since machine instructions don't change, the program pregion may be shared by many processes of the same executable.[134]
To save time and memory, the kernel may load only blocks of execution instructions from the disk drive, not the entire execution file completely.[133]
The kernel is responsible for translating virtual addresses into physical addresses. The kernel may request data from the memory controller and, instead, receive a page fault.[135] If so, the kernel accesses the memory management unit to populate the physical data region and translate the address.[136]
The kernel allocates memory from the heap upon request by a process.[63] When the process is finished with the memory, the process may request for it to be freed. If the process exits without requesting all allocated memory to be freed, then the kernel performs garbage collection to free the memory.
The kernel also ensures that a process only accesses its own memory, and not that of the kernel or other processes.[133]The kernel program should perform file system management.[133] The kernel has instructions to create, retrieve, update, and delete files.
The kernel program should perform device management.[133] The kernel provides programs to standardize and simplify the interface to the mouse, keyboard, disk drives, printers, and other devices. Moreover, the kernel should arbitrate access to a device if two processes request it at the same time.
The kernel program should perform network management.[137] The kernel transmits and receives packets on behalf of processes. One key service is to find an efficient route to the target system.
The kernel program should provide system level functions for programmers to use.[138]
Programmers access files through a relatively simple interface that in turn executes a relatively complicated low-level I/O interface. The low-level interface includes file creation, file descriptors, file seeking, physical reading, and physical writing.
Programmers create processes through a relatively simple interface that in turn executes a relatively complicated low-level interface.
Programmers perform date/time arithmetic through a relatively simple interface that in turn executes a relatively complicated low-level time interface.[139]
The kernel program should provide a communication channel between executing processes.[140] For a large software system, it may be desirable to engineer the system into smaller processes. Processes may communicate with one another by sending and receiving signals.Programmers access files through a relatively simple interface that in turn executes a relatively complicated low-level I/O interface. The low-level interface includes file creation, file descriptors, file seeking, physical reading, and physical writing.
Programmers create processes through a relatively simple interface that in turn executes a relatively complicated low-level interface.
Programmers perform date/time arithmetic through a relatively simple interface that in turn executes a relatively complicated low-level time interface.[139]Originally, operating systems were programmed in assembly; however, modern operating systems are typically written in higher-level languages like C, Objective-C, and Swift.[f]
A utility program is designed to aid system administration and software execution. Operating systems execute hardware utility programs to check the status of disk drives, memory, speakers, and printers.[141] A utility program may optimize the placement of a file on a crowded disk. System utility programs monitor hardware and network performance. When a metric is outside an acceptable range, a trigger alert is generated.[142]
Utility programs include compression programs so data files are stored on less disk space.[141] Compressed programs also save time when data files are transmitted over the network.[141] Utility programs can sort and merge data sets.[142] Utility programs detect computer viruses.[142]
A microcode program is the bottom-level interpreter that controls the data path of software-driven computers.[143]
(Advances in hardware have migrated these operations to hardware execution circuits.)[143] Microcode instructions allow the programmer to more easily implement the digital logic level[144]—the computer's real hardware. The digital logic level is the boundary between computer science and computer engineering.[145]
A logic gate is a tiny transistor that can return one of two signals: on or off.[146]
Having one transistor forms the NOT gate.
Connecting two transistors in series forms the NAND gate.
Connecting two transistors in parallel forms the NOR gate.
Connecting a NOT gate to a NAND gate forms the AND gate.
Connecting a NOT gate to a NOR gate forms the OR gate.These five gates form the building blocks of binary algebra—the digital logic functions of the computer.
Microcode instructions are mnemonics programmers may use to execute digital logic functions instead of forming them in binary algebra. They are stored in a central processing unit's (CPU) control store.[147]
These hardware-level instructions move data throughout the data path.
The micro-instruction cycle begins when the microsequencer uses its microprogram counter to fetch the next machine instruction from random-access memory.[148] The next step is to decode the machine instruction by selecting the proper output line to the hardware module.[149]
The final step is to execute the instruction using the hardware module's set of gates.
Instructions to perform arithmetic are passed through an arithmetic logic unit (ALU).[150] The ALU has circuits to perform elementary operations to add, shift, and compare integers. By combining and looping the elementary operations through the ALU, the CPU performs its complex arithmetic.
Microcode instructions move data between the CPU and the memory controller. Memory controller microcode instructions manipulate two registers. The memory address register is used to access each memory cell's address. The memory data register is used to set and read each cell's contents.[151]
Microcode instructions move data between the CPU and the many computer buses. The disk controller bus writes to and reads from hard disk drives. Data is also moved between the CPU and other functional units via the peripheral component interconnect express bus.[152]
"
4,https://en.wikipedia.org/wiki/Execution_(computing),"Code
Translation
Compiler
Compile time
Optimizing compiler
Intermediate representation (IR)
Execution
Runtime system
Runtime
Executable
Interpreter
Virtual machineCompiler
Compile time
Optimizing compilerCompile timeRuntime system
Runtime
Executable
Interpreter
Virtual machineRuntimeSource code
Object code
Bytecode
Machine code
MicrocodeAhead-of-time (AOT)
Just-in-time (JIT)
Tracing just-in-time
Compile and go system
Precompilation
Transcompilation
RecompilationTracing just-in-time
Compile and go systemAndroid Runtime (ART)
BEAM
Common Language Runtime (CLR) and Mono
CPython and PyPy
crt0
Java virtual machine (JVM)
LuaJIT
Objective-C and Swift's
V8 and Node.js
Zend EngineGNU Compiler Collection (GCC)
LLVM and ClangvteExecution in computer and software engineering is the process by which a computer or virtual machine reads and acts on the instructions of a computer program. Each instruction of a program is a description of a particular action which must be carried out, in order for a specific problem to be solved. Execution involves repeatedly following a 'fetch–decode–execute' cycle for each instruction done by control unit. As the executing machine follows the instructions, specific effects are produced in accordance with the semantics of those instructions. 
Programs for a computer may be executed in a batch process without human interaction or a user may type commands in an interactive session of an interpreter. In this case, the ""commands"" are simply program instructions, whose execution is chained together. 
The term run is used almost synonymously. A related meaning of both ""to run"" and ""to execute"" refers to the specific action of a user starting (or launching or invoking) a program, as in ""Please run the application.""
Prior to execution, a program must first be written. This is generally done in source code, which is then compiled at compile time (and statically linked at link time) to produce an executable. This executable is then invoked, most often by an operating system, which loads the program into memory (load time), possibly performs dynamic linking, and then begins execution by moving control to the entry point of the program; all these steps depend on the Application Binary Interface of the operating system. At this point execution begins and the program enters run time. The program then runs until it ends, either normal termination or a crash.
Executable code, an executable file, or an executable program, sometimes simply referred to as an executable or 
binary, is a list of instructions and data to cause a computer ""to perform indicated tasks according to encoded instructions"",[1] as opposed to a data file that must be interpreted (parsed) by a program to be meaningful. 
The exact interpretation depends upon the use. ""Instructions"" is traditionally taken to mean machine code instructions for a physical CPU.[2] In some contexts, a file containing scripting instructions (such as bytecode) may also be considered executable.
The context in which execution takes place is crucial. Very few programs execute on a bare machine. Programs usually contain implicit and explicit assumptions about resources available at the time of execution. Most programs execute within multitasking operating system and run-time libraries specific to the source language that provide crucial services not supplied directly by the computer itself. This supportive environment, for instance, usually decouples a program from direct manipulation of the computer peripherals, providing more general, abstract services instead.
In order for programs and interrupt handlers to work without interference and share the same hardware memory and access to the I/O system, in a multitasking operating systems running on a digital system with a single CPU/MCU it is required to have some sort of software and hardware facilities to keep track of an executing processes data (memory page addresses, registers etc.) and to save and recover them back to the state they were in before they were suspended. This is achieved by a context switching.[3]: 3.3 [4] The running programs are often assigned a Process Context IDentifiers (PCID).
In Linux-based operating systems, a set of data stored in registers is usually saved into a process descriptor in memory to implement switching of context.[3] PCIDs are also used.
A runtime system, also called runtime environment, primarily implements portions of an execution model.[clarification needed] This is not to be confused with the runtime lifecycle phase of a program, during which the runtime system is in operation. When treating the runtime system as distinct from the runtime environment (RTE), the first may be defined as a specific part of the application software (IDE) used for programming, a piece of software that provides the programmer a more convenient environment for running programs during their production (testing and similar), while the second (RTE) would be the very instance of an execution model being applied to the developed program which is itself then run in the aforementioned runtime system.
Most programming languages have some form of runtime system that provides an environment in which programs run. This environment may address a number of issues including the management of application memory, how the program accesses variables, mechanisms for passing parameters between procedures, interfacing with the operating system, and otherwise. The compiler makes assumptions depending on the specific runtime system to generate correct code. Typically the runtime system will have some responsibility for setting up and managing the stack and heap, and may include features such as garbage collection, threads or other dynamic features built into the language.[5]
The instruction cycle (also known as the fetch–decode–execute cycle, or simply the fetch-execute cycle) is the cycle that the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. It is composed of three main stages: the fetch stage, the decode stage, and the execute stage. 
In simpler CPUs, the instruction cycle is executed sequentially, each instruction being processed before the next one is started. In most modern CPUs, the instruction cycles are instead executed concurrently, and often in parallel, through an instruction pipeline: the next instruction starts being processed before the previous instruction has finished, which is possible because the cycle is broken up into separate steps.[6]
A system that executes a program is called an interpreter of the program. Loosely speaking, an interpreter directly executes a program. This contrasts with a language translator that converts a program from one language to another before it is executed.
A virtual machine (VM) is the virtualization/emulation of a computer system.  Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.
Virtual machines differ and are organized by their function, shown here:
System virtual machines (also termed full virtualization VMs) provide a substitute for a real machine. They provide functionality needed to execute entire operating systems. A hypervisor uses native execution to share and manage hardware, allowing for multiple environments which are isolated from one another, yet exist on the same physical machine. Modern hypervisors use hardware-assisted virtualization, virtualization-specific hardware, primarily from the host CPUs.
Process virtual machines are designed to execute computer programs in a platform-independent environment.Some virtual machine emulators, such as QEMU and video game console emulators, are designed to also emulate (or ""virtually imitate"") different system architectures thus allowing execution of software applications and operating systems written for another CPU or architecture. OS-level virtualization allows the resources of a computer to be partitioned via the kernel. The terms are not universally interchangeable.
"
5,https://en.wikipedia.org/wiki/Session_(computer_science),"In computer science and networking in particular, a session is a time-delimited two-way link, a practical (relatively high) layer in the TCP/IP protocol enabling interactive expression and information exchange between two or more communication devices or ends – be they computers, automated systems, or live active users (see login session). A session is established at a certain point in time, and then ‘torn down’ - brought to an end - at some later point. An established communication session may involve more than one message in each direction. A session is typically stateful, meaning that at least one of the communicating parties needs to hold current state information and save information about the session history to be able to communicate, as opposed to stateless communication, where the communication consists of independent requests with responses.
An established session is the basic requirement to perform a connection-oriented communication. A session also is the basic step to transmit in connectionless communication modes. However, any unidirectional transmission does not define a session.[1]
Communication Transport may be implemented as part of protocols and services at the application layer, at the session layer or at the transport layer in the OSI model.
Application layer examples:
HTTP sessions, which allow associating information with individual visitors
A telnet remote login session
Session layer example:
A Session Initiation Protocol (SIP) based Internet phone call
Transport layer example:
A TCP session, which is synonymous to a TCP virtual circuit, a TCP connection, or an established TCP socket.HTTP sessions, which allow associating information with individual visitors
A telnet remote login sessionA Session Initiation Protocol (SIP) based Internet phone callA TCP session, which is synonymous to a TCP virtual circuit, a TCP connection, or an established TCP socket.In the case of transport protocols that do not implement a formal session layer (e.g., UDP) or where sessions at the application layer are generally very short-lived (e.g., HTTP), sessions are maintained by a higher level program using a method defined in the data being exchanged.  For example, an HTTP exchange between a browser and a remote host may include an HTTP cookie which identifies state, such as a unique session ID, information about the user's preferences or authorization level.
HTTP/1.0 was thought to only allow a single request and response during one Web/HTTP Session. Protocol version HTTP/1.1 improved this by completing the Common Gateway Interface (CGI), making it easier to maintain the Web Session and supporting HTTP cookies and file uploads.
Most client-server sessions are maintained by the transport layer - a single connection for a single session. However each transaction phase of a Web/HTTP session creates a separate connection. Maintaining session continuity between phases requires a session ID. The session ID is embedded within the <A HREF> or <FORM> links of dynamic web pages so that it is passed back to the CGI. CGI then uses the session ID to ensure session continuity between transaction phases. One advantage of one connection-per-phase is that it works well over low bandwidth (modem) connections.
TCP sessions are typically implemented in software using child processes and/or multithreading, where a new process or thread is created when the computer establishes or joins a session. HTTP sessions are typically not implemented using one thread per session, but by means of a database with information about the state of each session. The advantage with multiple processes or threads is relaxed complexity of the software, since each thread is an instance with its own history and encapsulated variables. The disadvantage is large overhead in terms of system resources, and that the session may be interrupted if the system is restarted.
When a client may connect to any server in a cluster of servers, a special problem is encountered in maintaining consistency when the servers must maintain session state.  The client must either be directed to the same server for the duration of the session, or the servers must transmit server-side session information via a shared file system or database.  Otherwise, the client may reconnect to a different server than the one it started the session with, which will cause problems when the new server does not have access to the stored state of the old one.
Server-side sessions are handy and efficient, but can become difficult to handle in conjunction with load-balancing/high-availability systems and are not usable at all in some embedded systems with no storage. The load-balancing problem can be solved by using shared storage or by applying forced peering between each client and a single server in the cluster, although this can compromise system efficiency and load distribution.
A method of using server-side sessions in systems without mass-storage is to reserve a portion of RAM for storage of session data. This method is applicable for servers with a limited number of clients (e.g. router or access point with infrequent or disallowed access to more than one client at a time).
Client-side sessions use cookies and cryptographic techniques to maintain state without storing as much data on the server. When presenting a dynamic web page, the server sends the current state data to the client (web browser) in the form of a cookie. The client saves the cookie in memory or on disk. With each successive request, the client sends the cookie back to the server, and the server uses the data to ""remember"" the state of the application for that specific client and generate an appropriate response.
This mechanism may work well in some contexts; however, data stored on the client is vulnerable to tampering by the user or by software that has access to the client computer. To use client-side sessions where confidentiality and integrity are required, the following must be guaranteed:
To accomplish this, the server needs to encrypt the session data before sending it to the client, and modification of such information by any other party should be prevented via cryptographic means.
Transmitting state back and forth with every request is only practical when the size of the cookie is small. In essence, client-side sessions trade server disk space for the extra bandwidth that each web request will require. Moreover, web browsers limit the number and size of cookies that may be stored by a web site. To improve efficiency and allow for more session data, the server may compress the data before creating the cookie, decompressing it later when the cookie is returned by the client.
A session token is a unique identifier that is generated and sent from a server to a client to identify the current interaction session. The client usually stores and sends the token as an HTTP cookie and/or sends it as a parameter in GET or POST queries. The reason to use session tokens is that the client only has to handle the identifier—all session data is stored on the server (usually in a database, to which the client does not have direct access) linked to that identifier. Examples of the names that some programming languages use when naming their HTTP cookie include JSESSIONID (JSP), PHPSESSID (PHP), CGISESSID (CGI), and ASPSESSIONID (ASP).
In human–computer interaction, session management is the process of keeping track of a user's activity across sessions of interaction with the computer system.
Typical session management tasks in a desktop environment include keeping track of which applications are open and which documents each application has opened, so that the same state can be restored when the user logs out and logs in later. For a website, session management might involve requiring the user to re-login if the session has expired (i.e., a certain time limit has passed without user activity). It is also used to store information on the server-side between HTTP requests.
A desktop session manager is a program that can save and restore desktop sessions. A desktop session is all the windows currently running and their current content. Session management on Linux-based systems is provided by X session manager. On Microsoft Windows systems, session management is provided by the Session Manager Subsystem (smss.exe); user session functionality can be extended by third-party applications like twinsplay.
Session management is particularly useful in a web browser where a user can save all open pages and settings and restore them at a later date or on a different  computer (see data portability).
To help recover from a system or application crash, pages and settings can also be restored on next run. Google Chrome, Mozilla Firefox, Internet Explorer, OmniWeb and Opera are examples of web browsers that support session management. Session management is often managed through the application of cookies.
Hypertext Transfer Protocol (HTTP) is stateless. Session management is the technique used by the web developer to make the stateless HTTP protocol support session state. For example, once a user has been authenticated to the web server, the user's next HTTP request (GET or POST) should not cause the web server to ask for the user's account and password again. For a discussion of the methods used to accomplish this see HTTP cookie and Session ID
In situations where multiple web servers must share knowledge of session state (as is typical in a cluster environment) session information must be shared between the cluster nodes that are running web server software. Methods for sharing session state between nodes in a cluster include: multicasting session information to member nodes (see JGroups for one example of this technique), sharing session information with a partner node using distributed shared memory or memory virtualization, sharing session information between nodes using network sockets, storing session information on a shared file system such as a distributed file system or a global file system, or storing the session information outside the cluster in a database.
If session information is considered transient, volatile data that is not required for non-repudiation of transactions and does not contain data that is subject to compliance auditing (in the U.S. for example, see the Health Insurance Portability and Accountability Act and the Sarbanes–Oxley Act for examples of two laws that necessitate compliance auditing) then any method of storing session information can be used.  However, if session information is subject to audit compliance, consideration should be given to the method used for session storage, replication, and clustering.
In a service-oriented architecture, Simple Object Access Protocol or  SOAP messages constructed with Extensible Markup Language (XML) messages can be used by consumer applications to cause web servers to create sessions.
Just as HTTP is a stateless protocol, so is SMS.  As SMS became interoperable across rival networks in 1999,[2] and text messaging started its ascent towards becoming a ubiquitous global form of communication,[3] various enterprises became interested in using the SMS channel for commercial purposes.  Initial services did not require session management since they were only one-way communications (for example, in 2000, the first mobile news service was delivered via SMS in Finland).  Today, these applications are referred to as application-to-peer (A2P) messaging as distinct from peer-to-peer (P2P) messaging.  The development of interactive enterprise applications required session management, but because SMS is a stateless protocol as defined by the GSM standards,[4] early implementations were controlled client-side by having the end-users enter commands and service identifiers manually.
"
